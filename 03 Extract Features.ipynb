{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the datafiles we've created, extract out the details we're going to use to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "def set_update_start():\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 40\n",
    "    seconds_left = 0;\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    if progress > 0:\n",
    "        passed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        seconds_left = (passed_time / progress) - passed_time\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text, \" Remaining: \" + ':'.join(str(datetime.timedelta(seconds=seconds_left)).split('.')[:1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the description into tokens\n",
    "# lemmatize the tokens\n",
    "# create pairs of the tokens e.g. the|fat, fat|cat, cat|sat, sat|on etc\n",
    "# build an table of how often they appear in exploitable CVEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cves = pd.read_csv('data/cve_fulldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def build_ngrams(text):\n",
    "    ngrams = { 'bigrams': None, 'trigrams': None }\n",
    "    text = re.sub(r'([a-zA-Z])-([a-zA-Z])', r'\\1\\2', text) #cross-site => crosssite , E-commerce => ecommerce\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    monograms = []\n",
    "    corpus = nlp(text)\n",
    "    n_minus_one_word = ''\n",
    "    n_minus_two_word = ''\n",
    "    for word in corpus:\n",
    "        if not word.pos_ in ['SYM', 'PUNCT', 'ADP', 'DET', 'CCONJ', 'PART', 'NUM', 'SPACE', 'X']:\n",
    "            this_word = word.lemma_.lower()\n",
    "            if this_word[0] in GO:\n",
    "                if not n_minus_one_word == '':\n",
    "                    bigrams.append(n_minus_one_word + '|' + this_word)\n",
    "                if not n_minus_two_word == '':\n",
    "                    trigrams.append(n_minus_two_word + '|' + n_minus_one_word + '|' + this_word)\n",
    "                monograms.append(this_word)\n",
    "                n_minus_two_word = n_minus_one_word\n",
    "                n_minus_one_word = this_word\n",
    "    ngrams['monograms'] = monograms\n",
    "    ngrams['bigrams'] = bigrams\n",
    "    ngrams['trigrams'] = trigrams\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [########################################] 100.0%  Remaining: 0:00:00\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "monograms_row_accumulator = []\n",
    "bigrams_row_accumulator = []\n",
    "trigrams_row_accumulator = []\n",
    "counter_limit = 1000000000 # arbitrary very high number\n",
    "size = len(cves.index)\n",
    "\n",
    "set_update_start()\n",
    "for index, row in cves.iterrows():\n",
    "    if index < counter_limit: # limiter for testing\n",
    "        if (index % 100 == 0): \n",
    "            update_progress(index / size)\n",
    "        exploited = int(row['ExploitDB'] > 0 or row['Snort'] > 0)\n",
    "        ngrams = build_ngrams(row['Description'])\n",
    "        for token in ngrams['bigrams']:\n",
    "            new_row = { 'Token': token, 'Exploited': exploited, 'Not Exploited': abs(exploited -1) }\n",
    "            bigrams_row_accumulator.append(new_row)            \n",
    "        for token in ngrams['trigrams']:\n",
    "            new_row = { 'Token': token, 'Exploited': exploited, 'Not Exploited': abs(exploited -1) }\n",
    "            trigrams_row_accumulator.append(new_row)\n",
    "        for token in ngrams['monograms']:\n",
    "            new_row = { 'Token': token, 'Exploited': exploited, 'Not Exploited': abs(exploited -1) }\n",
    "            monograms_row_accumulator.append(new_row) \n",
    "        \n",
    "update_progress(1)\n",
    "token_monograms = pd.DataFrame(monograms_row_accumulator)\n",
    "token_bigrams = pd.DataFrame(bigrams_row_accumulator)\n",
    "token_trigrams = pd.DataFrame(trigrams_row_accumulator)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_monograms.to_csv('data/token_extract_monograms.csv',index=False)\n",
    "token_bigrams.to_csv('data/token_extract_bigrams.csv',index=False)\n",
    "token_trigrams.to_csv('data/token_extract_trigrams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "monograms = token_monograms.groupby(['Token'])['Exploited', 'Not Exploited'].sum()\n",
    "monograms = monograms[monograms['Exploited'] > 0]          # they need to tell us something\n",
    "monograms = monograms[monograms['Not Exploited'] > 0]      # they need to tell us something\n",
    "monograms.to_csv('data/token_frequencies_monograms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = token_bigrams.groupby(['Token'])['Exploited', 'Not Exploited'].sum()\n",
    "bigrams = bigrams[bigrams['Exploited'] > 0]          # they need to tell us something\n",
    "bigrams = bigrams[bigrams['Not Exploited'] > 0]      # they need to tell us something\n",
    "bigrams.to_csv('data/token_frequencies_bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = token_trigrams.groupby(['Token'])['Exploited', 'Not Exploited'].sum()\n",
    "trigrams = trigrams[trigrams['Exploited'] > 0]          # they need to tell us something\n",
    "trigrams = trigrams[trigrams['Not Exploited'] > 0]      # they need to tell us something\n",
    "trigrams.to_csv('data/token_frequencies_trigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
