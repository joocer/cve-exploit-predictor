{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the datafiles we've created, extract out the details we're going to use to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the description into tokens\n",
    "# create ngrams the tokens e.g. the|fat, fat|cat, cat|sat, sat|on etc\n",
    "# build an table of how often they appear in exploitable CVEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "start_time = None\n",
    "\n",
    "def set_update_start():\n",
    "    global start_time\n",
    "    start_time = None\n",
    "\n",
    "def update_progress(progress):\n",
    "    global start_time\n",
    "    if start_time == None:\n",
    "        start_time = datetime.datetime.now()\n",
    "    bar_length = 40\n",
    "    seconds_left = 0;\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    if progress > 0:\n",
    "        passed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        seconds_left = (passed_time / progress) - passed_time\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text, \" Remaining: \" + ':'.join(str(datetime.timedelta(seconds=seconds_left)).split('.')[:1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cves = pd.read_csv('data/cve_fulldata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - df: a Dataframe, chunkSize: the chunk size\n",
    "# output - a list of DataFrame\n",
    "# purpose - splits the DataFrame into smaller of max size chunkSize (last is smaller)\n",
    "def split_df(df, ratio=0.8): \n",
    "    listOfDf = list()\n",
    "    chunkSize = round(len(df) * ratio)\n",
    "    numberChunks = 2 #len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "valid_chars = string.ascii_lowercase + string.digits + ' \\n'\n",
    "\n",
    "def build_ngrams(text):\n",
    "    ngrams = { }\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c in valid_chars])\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    monograms = []\n",
    "    words = text.split()\n",
    "    n_minus_one_word = None\n",
    "    n_minus_two_word = None\n",
    "    for word in words:\n",
    "        if not n_minus_one_word == None:\n",
    "            bigrams.append(\"\".join([n_minus_one_word, '|', word]))\n",
    "        if not n_minus_two_word == None:\n",
    "            trigrams.append(\"\".join([n_minus_two_word, '|', n_minus_one_word, '|', word]))\n",
    "        monograms.append(word)\n",
    "        n_minus_two_word = n_minus_one_word\n",
    "        n_minus_one_word = word\n",
    "    ngrams['monograms'] = monograms\n",
    "    ngrams['bigrams'] = bigrams\n",
    "    ngrams['trigrams'] = trigrams\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monograms_row_accumulator = []\n",
    "bigrams_row_accumulator = []\n",
    "trigrams_row_accumulator = []\n",
    "counter = 10\n",
    "\n",
    "train_test = split_df(cves.sample(frac=1))\n",
    "train = train_test[0]\n",
    "size = len(train)\n",
    "\n",
    "set_update_start()\n",
    "for index, row in train.iterrows():\n",
    "    counter = counter + 1\n",
    "    if (counter % 100 == 0): \n",
    "        update_progress(counter / size)\n",
    "    snort = int(row['Snort'] > 0)\n",
    "    exploitdb = int(row['ExploitDB'] > 0)\n",
    "    exploitable = int(row['Snort'] > 0 or row['ExploitDB'] > 0)\n",
    "    ngrams = build_ngrams(row['Description'])\n",
    "    for token in ngrams['bigrams']:\n",
    "        new_row = { 'Token': token, 'Snort': snort, 'ExploitDB': exploitdb, 'Count': 1, 'Exploitable': exploitable }\n",
    "        bigrams_row_accumulator.append(new_row)            \n",
    "    for token in ngrams['trigrams']:\n",
    "        new_row = { 'Token': token, 'Snort': snort, 'ExploitDB': exploitdb, 'Count': 1, 'Exploitable': exploitable }\n",
    "        trigrams_row_accumulator.append(new_row)\n",
    "    for token in ngrams['monograms']:\n",
    "        new_row = { 'Token': token, 'Snort': snort, 'ExploitDB': exploitdb, 'Count': 1, 'Exploitable': exploitable }\n",
    "        monograms_row_accumulator.append(new_row) \n",
    "\n",
    "update_progress(1)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_monograms = pd.DataFrame(monograms_row_accumulator)\n",
    "token_monograms.to_csv('data/token_extract_monograms.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monograms = token_monograms.groupby(['Token'])['Snort', 'ExploitDB', 'Count', 'Exploitable'].sum()\n",
    "monograms.to_csv('model/token_frequencies_monograms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bigrams = pd.DataFrame(bigrams_row_accumulator)\n",
    "token_bigrams.to_csv('data/token_extract_bigrams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = token_bigrams.groupby(['Token'])['Snort', 'ExploitDB', 'Count', 'Exploitable'].sum()\n",
    "bigrams.to_csv('model/token_frequencies_bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_trigrams = pd.DataFrame(trigrams_row_accumulator)\n",
    "token_trigrams.to_csv('data/token_extract_trigrams.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = token_trigrams.groupby(['Token'])['Snort', 'ExploitDB', 'Count', 'Exploitable'].sum()\n",
    "trigrams.to_csv('model/token_frequencies_trigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test[1].to_csv('model/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
